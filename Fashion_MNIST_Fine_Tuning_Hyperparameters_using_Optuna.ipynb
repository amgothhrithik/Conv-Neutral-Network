{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOytYqWt5M5wqhBjGeU0iFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amgothhrithik/Conv-Neutral-Network/blob/main/Fashion_MNIST_Fine_Tuning_Hyperparameters_using_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using device:{device}\")\n",
        "\n",
        "!kaggle datasets download -d zalando-research/fashionmnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VphCV8tolYVp",
        "outputId": "886ac1ba-ff7d-4acb-e723-d7b47fd923da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device:cuda\n",
            "Dataset URL: https://www.kaggle.com/datasets/zalando-research/fashionmnist\n",
            "License(s): other\n",
            "Downloading fashionmnist.zip to /content\n",
            " 92% 63.0M/68.8M [00:01<00:00, 74.5MB/s]\n",
            "100% 68.8M/68.8M [00:01<00:00, 69.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip fashionmnist.zip\n",
        "import pandas as pd\n",
        "\n",
        "# Load training data\n",
        "train_df = pd.read_csv('fashion-mnist_train.csv')\n",
        "# Load testing data\n",
        "test_df = pd.read_csv('fashion-mnist_test.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdqqa-E2kqb_",
        "outputId": "dd76590d-bd0b-4ce9-fc4a-5bcc15dc9977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fashionmnist.zip\n",
            "  inflating: fashion-mnist_test.csv  \n",
            "  inflating: fashion-mnist_train.csv  \n",
            "  inflating: t10k-images-idx3-ubyte  \n",
            "  inflating: t10k-labels-idx1-ubyte  \n",
            "  inflating: train-images-idx3-ubyte  \n",
            "  inflating: train-labels-idx1-ubyte  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(train_df.head())\n",
        "print(train_df.shape, test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5oU_oUDk7Rs",
        "outputId": "aafceb01-bf87-42d7-d424-062c31e655eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 785) (10000, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "AYdxM720lQqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "Y_train=train_df.iloc[:,0].values\n",
        "X_train=train_df.iloc[:,1:].values/255.0\n",
        "\n",
        "Y_test=test_df.iloc[:,0].values\n",
        "X_test=test_df.iloc[:,1:].values/255.0\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxnUNPu9mxOz",
        "outputId": "8129216c-e0c0-4950-8d43-43562c0e4249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "(60000,)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,features,labels):\n",
        "    self.features=torch.tensor(features,dtype=torch.float32)\n",
        "    self.labels=torch.tensor(labels,dtype=torch.long)#.reshape(-1, 1)\n",
        "  def __len__(self):\n",
        "    return self.features.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "\n",
        "\n",
        "    #tranformation in this\n",
        "    return self.features[index],self.labels[index]"
      ],
      "metadata": {
        "id": "J_aVuOCVnK0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=CustomDataset(X_train,Y_train)\n",
        "test_dataset=CustomDataset(X_test,Y_test)"
      ],
      "metadata": {
        "id": "zJhG8rA_nNu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "train_dataloader= torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True,pin_memory=True)\n",
        "test_dataloader= torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True,pin_memory=True)"
      ],
      "metadata": {
        "id": "mDCmdLHYjPDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "layer=[X_train.shape[1],128,64,10]"
      ],
      "metadata": {
        "id": "s6gDlaG4nQRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Architecture"
      ],
      "metadata": {
        "id": "UU_Fpa2bvARE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN(nn.Module):\n",
        "  def __init__(self,layer):\n",
        "    super().__init__()\n",
        "    self.layer_size=len(layer)\n",
        "\n",
        "    self.network=nn.ModuleDict()\n",
        "\n",
        "    for i in range(1,self.layer_size):\n",
        "      self.network[f\"W{i}\"]=nn.Linear(layer[i-1],layer[i])\n",
        "\n",
        "      if i<self.layer_size-1:\n",
        "        self.network[f\"batch_Norm{i}\"]=nn.BatchNorm1d(layer[i])\n",
        "        self.network[f\"drop_out{i}\"]=nn.Dropout(p=0.3)\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "    for i in range(1,self.layer_size):\n",
        "      X=self.network[f\"W{i}\"](X)\n",
        "\n",
        "      if i<self.layer_size-1:\n",
        "        X=self.network[f\"batch_Norm{i}\"](X)\n",
        "        X=torch.relu(X)\n",
        "        X=self.network[f\"drop_out{i}\"](X)\n",
        "\n",
        "    return X\n",
        "model=ANN(layer)\n",
        "model=model.to(device)"
      ],
      "metadata": {
        "id": "-o2UhQDyipMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff-YdsxCwap-",
        "outputId": "3cedebe5-4bdb-48ad-cfb1-e97d05703d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ANN(\n",
              "  (network): ModuleDict(\n",
              "    (W1): Linear(in_features=784, out_features=128, bias=True)\n",
              "    (batch_Norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop_out1): Dropout(p=0.3, inplace=False)\n",
              "    (W2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (batch_Norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (drop_out2): Dropout(p=0.3, inplace=False)\n",
              "    (W3): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Function for a Dataset"
      ],
      "metadata": {
        "id": "n9LnhR5-vMgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(dataloader):\n",
        "  acc=0\n",
        "  model.eval()\n",
        "  for batch_features,batch_labels in dataloader:\n",
        "    batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      y_pred1=model(batch_features)\n",
        "      y_pred11=torch.max(y_pred1, 1)[1]\n",
        "      acc+=(y_pred11==batch_labels).float().mean()\n",
        "  return acc/len(dataloader)"
      ],
      "metadata": {
        "id": "tnopPZN1l7LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters Initization"
      ],
      "metadata": {
        "id": "KJseAVYtvkxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lr=0.005\n",
        "epochs=100\n",
        "loss_func=nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
        "#optim.SGD(model.parameters(),lr,momentum=0.9,weight_decay=1e-2)"
      ],
      "metadata": {
        "id": "utB2RrEYpP88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_model(epochs,dataloader,loss_func,optimizer,lr):\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs+1):\n",
        "    loss_epoch=0\n",
        "    for batch_features,batch_labels in dataloader:\n",
        "      batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "\n",
        "      y_pred=model(batch_features)\n",
        "\n",
        "      loss_batch=loss_func(y_pred,batch_labels)\n",
        "      loss_epoch+=loss_batch.item()/len(dataloader)\n",
        "      optimizer.zero_grad()\n",
        "      loss_batch.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  return model.parameters()\n",
        "\n"
      ],
      "metadata": {
        "id": "StOFv-r91krt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "loss=[]\n",
        "for epoch in range(epochs+1):\n",
        "  loss_epoch=0\n",
        "  for batch_features,batch_labels in train_dataloader:\n",
        "    batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "\n",
        "    y_pred=model(batch_features)\n",
        "\n",
        "    loss_batch=loss_func(y_pred,batch_labels)\n",
        "    loss_epoch+=loss_batch.item()/len(train_dataloader)\n",
        "    optimizer.zero_grad()\n",
        "    loss_batch.backward()\n",
        "    optimizer.step()\n",
        "  loss.append(loss_epoch)\n",
        "  if (epoch ) % (epochs//5) == 0:\n",
        "    print(f\"Epoch {epoch} | Loss: {loss_epoch} |  Accuracy on training_data:{ predict(train_dataloader)} | Accuracy on test_data:{ predict(test_dataloader)}\")\n",
        "    print(\"=\"*130)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chCjuKlOi0gp",
        "outputId": "213b4968-95a9-4403-c8a4-9a3d57823110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.6986503779570257 |  Accuracy on training_data:0.8165833353996277 | Accuracy on test_data:0.8152955174446106\n",
            "==================================================================================================================================\n",
            "Epoch 20 | Loss: 0.4115243405262628 |  Accuracy on training_data:0.8405333161354065 | Accuracy on test_data:0.8406549096107483\n",
            "==================================================================================================================================\n",
            "Epoch 40 | Loss: 0.41338149827718684 |  Accuracy on training_data:0.86326664686203 | Accuracy on test_data:0.8563298583030701\n",
            "==================================================================================================================================\n",
            "Epoch 60 | Loss: 0.41588735479116457 |  Accuracy on training_data:0.8577666878700256 | Accuracy on test_data:0.8512380123138428\n",
            "==================================================================================================================================\n",
            "Epoch 80 | Loss: 0.41552710200548204 |  Accuracy on training_data:0.8507166504859924 | Accuracy on test_data:0.8456469774246216\n",
            "==================================================================================================================================\n",
            "Epoch 100 | Loss: 0.4139802326798434 |  Accuracy on training_data:0.8555166721343994 | Accuracy on test_data:0.8513378500938416\n",
            "==================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning Hyperparameters using Optuna"
      ],
      "metadata": {
        "id": "-RC3G8iVpsqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architecture**"
      ],
      "metadata": {
        "id": "cq9c4bMcv2tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MY_ANN(nn.Module):\n",
        "  def __init__(self,input_dim,output_dim,num_hidden_layers,neurons_per_layer,dropout_rate):\n",
        "    super().__init__()\n",
        "    layer=[]\n",
        "    for i in range(num_hidden_layers):\n",
        "      layer.append(nn.Linear(input_dim,neurons_per_layer))\n",
        "\n",
        "      layer.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "      layer.append(nn.ReLU())\n",
        "      layer.append(nn.Dropout(p=dropout_rate))\n",
        "      input_dim=neurons_per_layer\n",
        "    layer.append(nn.Linear(input_dim,output_dim))\n",
        "\n",
        "    self.network=nn.Sequential(*layer)\n",
        "  def forward(self,X):\n",
        "    return self.network(X)"
      ],
      "metadata": {
        "id": "WXu2gmwQXoTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective Function"
      ],
      "metadata": {
        "id": "ju7b_4e5pq_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Objective function\n",
        "def objective(trial):\n",
        "\n",
        "  #next hyperparameter values from search space\n",
        "  num_hidden_layers=trial.suggest_int(\"num_hidden_layers\",1,5)\n",
        "  neurons_per_layer=trial.suggest_int(\"neurons_per_layer\",16,128,step=8)\n",
        "  epochs=trial.suggest_int(\"epochs\",10,60,step=10)\n",
        "  lr=trial.suggest_float(\"lr\",1e-5,1e-1,log=True)\n",
        "  dropout_rate=trial.suggest_float(\"dropout_rate\",0.1,0.4,step=0.05)\n",
        "  batch_size=trial.suggest_categorical(\"batch_size\",[16,32,64,128])\n",
        "  optimizer=trial.suggest_categorical('optimizer',[\"SGD\",\"Adam\",\"RMSprop\"])\n",
        "  wt_decay=trial.suggest_float(\"wt_decay\",1e-5,1e-1,log=True)\n",
        "\n",
        "\n",
        "\n",
        "  train_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
        "  test_dataloader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
        "\n",
        "  # input init\n",
        "  input_dim=784\n",
        "  output_dim=10\n",
        "\n",
        "  #Model init\n",
        "  model=MY_ANN(input_dim,output_dim,num_hidden_layers,neurons_per_layer,dropout_rate)\n",
        "  model.to(device)\n",
        "\n",
        "  #loss function\n",
        "  loss_func=nn.CrossEntropyLoss()\n",
        "  #optimizer selection\n",
        "  if optimizer==\"Adam\":\n",
        "    optimizer=optim.Adam(model.parameters(),lr,weight_decay=wt_decay)\n",
        "  elif optimizer==\"SGD\":\n",
        "    optimizer=optim.SGD(model.parameters(),lr,momentum=0.9,weight_decay=wt_decay)\n",
        "  else:\n",
        "    optimizer=optim.RMSprop(model.parameters(),lr,weight_decay=wt_decay)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #training loop\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    for batch_features,batch_labels in train_dataloader:\n",
        "      batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "      y_pred=model(batch_features)\n",
        "\n",
        "      loss_batch=loss_func(y_pred,batch_labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss_batch.backward()\n",
        "      optimizer.step()\n",
        "  #evaluation\n",
        "  acc=0\n",
        "  model.eval()\n",
        "  for batch_features,batch_labels in test_dataloader:\n",
        "    batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      y_pred1=model(batch_features)\n",
        "      y_pred11=torch.max(y_pred1, 1)[1]\n",
        "      #print(y_pred11)\n",
        "      acc+=(y_pred11==batch_labels).float().mean()\n",
        "  return acc/len(test_dataloader)"
      ],
      "metadata": {
        "id": "qmsgIjXKWNYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "njseXf0ycYrz",
        "outputId": "8aceca57-634e-44c8-bfa1-0b6f346466a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "study=optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective,n_trials=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEIUSiQTce-G",
        "outputId": "af262a24-2acb-4294-870e-f47cbf00fe9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-04 07:00:29,753] A new study created in memory with name: no-name-7bbd1358-f6cf-49dc-9828-535b84ef851e\n",
            "[I 2025-02-04 07:04:21,594] Trial 0 finished with value: 0.8850517868995667 and parameters: {'num_hidden_layers': 5, 'neurons_per_layer': 104, 'epochs': 40, 'lr': 0.001226281358718413, 'dropout_rate': 0.4, 'batch_size': 64, 'optimizer': 'Adam', 'wt_decay': 0.00011017284119743356}. Best is trial 0 with value: 0.8850517868995667.\n",
            "[I 2025-02-04 07:07:40,466] Trial 1 finished with value: 0.877587616443634 and parameters: {'num_hidden_layers': 2, 'neurons_per_layer': 64, 'epochs': 50, 'lr': 0.0029520968445556754, 'dropout_rate': 0.4, 'batch_size': 64, 'optimizer': 'Adam', 'wt_decay': 0.0001018597179025824}. Best is trial 0 with value: 0.8850517868995667.\n",
            "[I 2025-02-04 07:13:31,798] Trial 2 finished with value: 0.8064000010490417 and parameters: {'num_hidden_layers': 3, 'neurons_per_layer': 112, 'epochs': 30, 'lr': 0.019105849541904203, 'dropout_rate': 0.35, 'batch_size': 16, 'optimizer': 'SGD', 'wt_decay': 0.002726763852898359}. Best is trial 0 with value: 0.8850517868995667.\n",
            "[I 2025-02-04 07:15:05,767] Trial 3 finished with value: 0.8703224658966064 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 96, 'epochs': 30, 'lr': 0.07764876033209309, 'dropout_rate': 0.25, 'batch_size': 64, 'optimizer': 'SGD', 'wt_decay': 0.00033510860600063984}. Best is trial 0 with value: 0.8850517868995667.\n",
            "[I 2025-02-04 07:16:52,134] Trial 4 finished with value: 0.8732085824012756 and parameters: {'num_hidden_layers': 2, 'neurons_per_layer': 64, 'epochs': 30, 'lr': 0.00031586843437853235, 'dropout_rate': 0.35, 'batch_size': 64, 'optimizer': 'SGD', 'wt_decay': 3.277819434319886e-05}. Best is trial 0 with value: 0.8850517868995667.\n",
            "[I 2025-02-04 07:17:23,497] Trial 5 finished with value: 0.8608678579330444 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 128, 'epochs': 10, 'lr': 0.0007947087037672069, 'dropout_rate': 0.35, 'batch_size': 64, 'optimizer': 'Adam', 'wt_decay': 0.0011659497900099853}. Best is trial 0 with value: 0.8850517868995667.\n",
            "[I 2025-02-04 07:18:33,008] Trial 6 finished with value: 0.6878955960273743 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 64, 'epochs': 30, 'lr': 0.013094267214361702, 'dropout_rate': 0.1, 'batch_size': 128, 'optimizer': 'Adam', 'wt_decay': 0.043800670011706906}. Best is trial 0 with value: 0.8850517868995667.\n",
            "[I 2025-02-04 07:19:47,710] Trial 7 finished with value: 0.8780657052993774 and parameters: {'num_hidden_layers': 2, 'neurons_per_layer': 56, 'epochs': 30, 'lr': 4.402931707221304e-05, 'dropout_rate': 0.2, 'batch_size': 128, 'optimizer': 'RMSprop', 'wt_decay': 5.618744122624204e-05}. Best is trial 0 with value: 0.8850517868995667.\n",
            "[I 2025-02-04 07:20:32,889] Trial 8 finished with value: 0.844343364238739 and parameters: {'num_hidden_layers': 1, 'neurons_per_layer': 64, 'epochs': 20, 'lr': 1.476819785838677e-05, 'dropout_rate': 0.2, 'batch_size': 128, 'optimizer': 'Adam', 'wt_decay': 0.06086536245172869}. Best is trial 0 with value: 0.8850517868995667.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uySz3WPajiet",
        "outputId": "64d3766d-ae22-4c44-9226-c19c631aeb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8664952516555786"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Using CNN  Architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "z2U9jIA-xRrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define data augmentations for the training dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "uZM3gyYdJN5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,features,labels,transform=None):\n",
        "    self.features=torch.tensor(features,dtype=torch.float32).reshape(-1,1,28,28)\n",
        "    self.labels=torch.tensor(labels,dtype=torch.long)#.reshape(-1, 1)\n",
        "    self.tranformer=transform\n",
        "  def __len__(self):\n",
        "    return self.features.shape[0]\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    #tranformation\n",
        "    if self.tranformer:\n",
        "      features = self.transformer(features.squeeze(0).numpy())\n",
        "    #tranformation in this\n",
        "    return self.features[index],self.labels[index]"
      ],
      "metadata": {
        "id": "1My56zKryF9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=CustomDataset(X_train,Y_train,transform=train_transform)\n",
        "test_dataset=CustomDataset(X_test,Y_test,transform=test_transform)"
      ],
      "metadata": {
        "id": "P2YHGtAq4p8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self,num_cnn_layers,filter_size,num_fc_layers,fc_neurons,dropout_rate):\n",
        "    super().__init__()\n",
        "\n",
        "    # Convolutional layers\n",
        "    cnn_layer=[]\n",
        "    low=3   # CIFAR-10 images have 3 channels (RGB)\n",
        "    for i in range(num_cnn_layers):\n",
        "      cnn_layer.append(nn.Conv2d(low,filter_size[i],kernel_size=5,padding=\"same\"))\n",
        "      cnn_layer.append(nn.ReLU())\n",
        "      cnn_layer.append(nn.BatchNorm2d(filter_size[i]))\n",
        "      cnn_layer.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "      low=filter_size[i]\n",
        "\n",
        "    # Fully connected layers\n",
        "    ann_layer=[nn.Flatten()]\n",
        "\n",
        "    for i in range(num_fc_layers-1):\n",
        "\n",
        "      ann_layer.append(nn.Linear(fc_neurons[i],fc_neurons[i+1]))\n",
        "      ann_layer.append(nn.BatchNorm1d(fc_neurons[i+1]))\n",
        "      ann_layer.append(nn.ReLU())\n",
        "      ann_layer.append(nn.Dropout(p=dropout_rate))\n",
        "    # 10 classes for CIFAR-10\n",
        "    ann_layer.append(nn.Linear(fc_neurons[-1],10))\n",
        "\n",
        "\n",
        "    self.cnn_network=nn.Sequential(*cnn_layer)\n",
        "    self.fc_network=nn.Sequential(*ann_layer)\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "    X=self.cnn_network(X)\n",
        "    return self.fc_network(X)\n"
      ],
      "metadata": {
        "id": "uuNO6jlK0pG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def objective(trial):\n",
        "\n",
        "  #hyperparameter to be tuned  from search space\n",
        "  num_cnn_layers=trial.suggest_int(\"num_cnn_layers\",1,3)\n",
        "  # Filter sizes (with increasing sizes as layers increase)\n",
        "  filter_size=[]\n",
        "  filter_val=trial.suggest_int(f\"filter\",5,16)\n",
        "  for i in range(num_cnn_layers):\n",
        "    filter_size.append(filter_val)\n",
        "    filter_val=filter_val*2\n",
        "\n",
        "  filter_val = trial.suggest_int(f\"filter\", 5, 16)\n",
        "  filter_size = [filter_val := filter_val * 2 if i > 0 else filter_val for i in range(num_cnn_layers)]\n",
        "\n",
        "  num_fc_layers=trial.suggest_int(\"num_fc_layers\",2,5)\n",
        "\n",
        "\n",
        "\n",
        "  output_size = 28\n",
        "  for _ in range(num_cnn_layers):\n",
        "      output_size = output_size // 2  # Since MaxPool halves the size\n",
        "  fc_neurons=[filter_size[-1] * output_size * output_size]\n",
        "  steps=32\n",
        "  high_fc=((fc_neurons[-1]/4)//steps)*steps\n",
        "\n",
        "\n",
        "\n",
        "  low_fc=((high_fc/10)//steps)*steps\n",
        "  neurons=trial.suggest_int(f\"neurons_per_layer\",low_fc,high_fc,step=steps)\n",
        "  for i in range(num_fc_layers-1):\n",
        "\n",
        "    fc_neurons.append(neurons)\n",
        "    neurons=max(steps,neurons//2)\n",
        "\n",
        " # print(\"num_cnn_layers\",num_cnn_layers,\"\\n\",\"filter_size\",filter_size,\"\\n\",\"num_fc_layers\",\"\\n\",num_fc_layers,\"\\n\",\"fc_neurons\",fc_neurons)\n",
        "\n",
        "\n",
        "  # Additional hyperparameters\n",
        "  dropout_rate=trial.suggest_float(\"dropout_rate\",0.1,0.6,step=0.1)\n",
        "  batch_size=trial.suggest_categorical(\"batch_size\",[32,64,128])\n",
        "  optimizer=trial.suggest_categorical('optimizer',[\"SGD\",\"Adam\",\"RMSprop\"])\n",
        "  wt_decay=trial.suggest_float(\"wt_decay\",1e-5,1e-1,log=True)\n",
        "  epochs=trial.suggest_int(\"epochs\",10,30,step=5)\n",
        "  lr=trial.suggest_float(\"lr\",1e-5,1e-1,log=True)\n",
        "\n",
        "  # Build the model\n",
        "  model=CNN(num_cnn_layers,filter_size,num_fc_layers,fc_neurons,dropout_rate)\n",
        "  model.to(device)\n",
        "\n",
        "  #loss function\n",
        "  loss_func=nn.CrossEntropyLoss()\n",
        "  #optimizer selection\n",
        "  if optimizer==\"Adam\":\n",
        "    optimizer=optim.Adam(model.parameters(),lr,weight_decay=wt_decay)\n",
        "  elif optimizer==\"SGD\":\n",
        "    optimizer=optim.SGD(model.parameters(),lr,momentum=0.9,weight_decay=wt_decay)\n",
        "  else:\n",
        "    optimizer=optim.RMSprop(model.parameters(),lr,weight_decay=wt_decay)\n",
        "\n",
        "  # Data Loaders\n",
        "  trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
        "  testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #training loop\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    for batch_features,batch_labels in trainloader:\n",
        "      batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "      y_pred=model(batch_features)\n",
        "\n",
        "      loss_batch=loss_func(y_pred,batch_labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss_batch.backward()\n",
        "      optimizer.step()\n",
        "  #evaluation\n",
        "  acc=0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_features,batch_labels in testloader:\n",
        "      batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "\n",
        "      y_pred1=model(batch_features)\n",
        "      _, predicted=torch.max(y_pred1, 1)[1]\n",
        "      #print(y_pred11)\n",
        "      acc+=(predicted == batch_labels).float().mean().item()\n",
        "  return acc/len(testloader)"
      ],
      "metadata": {
        "id": "YBqzgYKw3OAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_val=6\n",
        "\n",
        "filter_size = [filter_val := int(filter_val * 1.5) if i > 0 else filter_val for i in range(4)]"
      ],
      "metadata": {
        "id": "wgEVmzHG7XyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgCNF-Q37gk8",
        "outputId": "97e702f7-21ff-402a-9ecd-b917064393e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 9, 13, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}